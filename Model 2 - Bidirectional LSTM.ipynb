{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b6ea24-566a-4fb9-9a04-1bbe6f9067eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from utils import Attention\n",
    "from keras.layers import Embedding\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7a7c84-510a-4ebd-baa0-5cc555ac35dd",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8362ba2f-0909-4f78-a3d6-e0e3b1667341",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train-lyrics.csv')\n",
    "test_df = pd.read_csv('test-lyrics.csv')\n",
    "\n",
    "joined_genres = pd.concat([df['genre'], test_df['genre']]).reset_index(drop=True)\n",
    "cat_labels = []\n",
    "for genre in joined_genres:\n",
    "    if genre == \"country\": cat_labels.append(0)\n",
    "    elif genre == \"pop\": cat_labels.append(1)\n",
    "    elif genre == \"r-b\": cat_labels.append(2)\n",
    "    elif genre == \"rock\": cat_labels.append(3)\n",
    "    elif genre == \"rap\": cat_labels.append(4)\n",
    "\n",
    "texts = pd.concat([df['input texts'], test_df['input texts']]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c5e797-dd59-4cfc-badb-dde5c3d4e921",
   "metadata": {},
   "source": [
    "# Loading in Pre-trained Word Vectors (GLoVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc182f6-dbe4-4a52-80a6-3d99826c147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(file):\n",
    "    \n",
    "    f = open(file, 'r', encoding='utf8')\n",
    "    glove_model = {}\n",
    "    \n",
    "    for line in f:\n",
    "        split_lines = line.split()\n",
    "        if len(split_lines) > 1:\n",
    "            word = split_lines[0]\n",
    "            word_embedding = np.array([float(value) for value in split_lines[1:]])\n",
    "            \n",
    "            glove_model[word] = word_embedding\n",
    "    \n",
    "    print(len(glove_model), \" words loaded\")\n",
    "    return glove_model\n",
    "\n",
    "glove_path = '../glove.6B.200d.txt'\n",
    "\n",
    "glove_pretrained = load_glove(glove_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9497c1fa-c1df-4fe3-99ef-d4ade68b5c30",
   "metadata": {},
   "source": [
    "### Creating Embedding Matrix from GLoVE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3872e88-b57b-4b5a-9326-b81e5e79d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 200\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS, \n",
    "                      filters='â€™()``''')\n",
    "\n",
    "# Create vocabulary from lyrics data\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "vocab_size = len(word_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e18649-20c1-44cc-b623-da8c99db9f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "# Creating our data matrix\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Creating label matrix \n",
    "labels = tf.keras.utils.to_categorical(cat_labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9921e812-2b90-4df1-9408-636313543ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating our embeddings matrix \n",
    "\n",
    "EMBEDDING_DIM = 200\n",
    "embeddings_matrix = np.zeros((vocab_size + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = glove_pretrained.get(word)\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        if embedding_vector is not None:\n",
    "            embeddings_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        \n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7244bcfe-311a-4bc9-8aa8-acb8e8411aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "[embeddings_matrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94abc372-7d5f-4582-a1e4-d5ea6d78abf9",
   "metadata": {},
   "source": [
    "## Model Creation and Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccba7974-a32b-4a34-be51-030f591dea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, \n",
    "                                                    labels, \n",
    "                                                    stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5160c949-57dc-4f62-b97e-88b1450b4a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5605089b-ec49-4e5a-b6e3-b2f847dbb72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7fa332-eb27-447d-b380-fa04e45ada82",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = y_train.shape[1]\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618b2544-592e-4762-9b11-4fd5e01c1f9d",
   "metadata": {},
   "source": [
    "# Baseline Model 2 - Bidirectional LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11f3e1f-dadf-4ebe-839e-8c226b8490ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(create_fn):\n",
    "    \n",
    "    model = create_fn()\n",
    "    \n",
    "    history = model.fit(X_train, y_train, \n",
    "                        epochs=10, \n",
    "                        batch_size=20, \n",
    "                        validation_data=(X_test, y_test),\n",
    "                        verbose=1)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829b3947-740a-4f02-a9d5-b6608ff17074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_training_df(history_dict, num_epochs):\n",
    "    epochs_index = pd.Index(data=list(range(1, num_epochs + 1)), name=\"epoch\")\n",
    "    out_df = pd.DataFrame(history_dict).set_index(epochs_index)\n",
    "    \n",
    "    return out_df\n",
    "\n",
    "def plot_model_performance(df, model_name):\n",
    "    \n",
    "    df_one = df[['loss', 'val_loss']]\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    sns.lineplot(data=df_one)\n",
    "    plt.title(model_name + \" Loss by Epochs\");\n",
    "    plt.show();\n",
    "    \n",
    "    df_two = df[['accuracy', 'val_accuracy']]\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    sns.lineplot(data=df_two)\n",
    "    plt.title(model_name + \" Accuracy by Epochs\");\n",
    "    plt.show();\n",
    "    \n",
    "def evaluate_model(model):\n",
    "    \n",
    "    return model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087c8508-5f63-436b-989e-14f70e7076e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(rnn_units=200,\n",
    "                 optimizer=tf.keras.optimizers.Adam(), \n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                 metrics=[\"accuracy\"]):\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding_layer = Embedding(vocab_size + 1, \n",
    "                                EMBEDDING_DIM, \n",
    "                                weights=[embeddings_matrix], \n",
    "                                input_length=MAX_SEQUENCE_LENGTH, \n",
    "                                trainable=False) \n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    model.add(layers.Input(shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "                           dtype=\"int32\"))\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "    \n",
    "    model.add(layers.Bidirectional(layers.LSTM(rnn_units)))\n",
    "    \n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(layers.Dense(num_labels, \n",
    "                           activation=\"softmax\"))\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=loss, \n",
    "                  metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e1fbf9-4a51-4391-9abb-563bcd3e0b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_2, baseline_model_2_history = fit_model(create_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3970f057-9ff0-4004-877d-3e554ca1ff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_2_df = create_model_training_df(baseline_model_2_history.history, 10)\n",
    "plot_model_performance(baseline_model_2_df, \"Bidirectional LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45349a95-f0c9-48e4-bda6-52eaac05729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(baseline_model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc6cac-a7e7-46cb-9354-13ecbf5839fe",
   "metadata": {},
   "source": [
    "# Improving Model 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbea653-9b5d-4119-84e8-831e86160e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_two(rnn_units=200,\n",
    "                     optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                     metrics=[\"accuracy\"]):\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding_layer = Embedding(vocab_size + 1, \n",
    "                                EMBEDDING_DIM, \n",
    "                                weights=[embeddings_matrix], \n",
    "                                input_length=MAX_SEQUENCE_LENGTH, \n",
    "                                trainable=True) \n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    model.add(layers.Input(shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "                           dtype=\"int32\"))\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "    \n",
    "    model.add(layers.Bidirectional(layers.LSTM(rnn_units)))\n",
    "    \n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(layers.Dense(num_labels, \n",
    "                           activation=\"softmax\"))\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=loss, \n",
    "                  metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eb2a36-47db-4244-a6b1-8668ec19cdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_model_2, improved_model_2_history = fit_model(create_model_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97282511-6eeb-4753-aab9-5107b71f7571",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_model_2_df = create_model_training_df(improved_model_2_history.history, 10)\n",
    "plot_model_performance(improved_model_2_df, \"Improved Bidirectional LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1759128-4d4c-4eb7-883d-89ed7c2f7b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(improved_model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10698971-f2c6-40dd-a8cb-7e952551cac5",
   "metadata": {},
   "source": [
    "# Improving Model 2 Pt. 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8344332-ef02-48c1-ba9e-b9d2fd1570a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_two_pt2(rnn_units=200,\n",
    "                         optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                         loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                         metrics=[\"accuracy\"]):\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding_layer = Embedding(vocab_size + 1, \n",
    "                                EMBEDDING_DIM, \n",
    "                                weights=[embeddings_matrix], \n",
    "                                input_length=MAX_SEQUENCE_LENGTH, \n",
    "                                trainable=True) \n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    model.add(layers.Input(shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "                           dtype=\"int32\"))\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "    \n",
    "    model.add(layers.Bidirectional(layers.LSTM(rnn_units, \n",
    "                                               return_sequences=True)))\n",
    "    \n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    \n",
    "    model.add(layers.LSTM(rnn_units))\n",
    "    \n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    \n",
    "    model.add(layers.Dense(num_labels, \n",
    "                           activation=\"softmax\", \n",
    "                           name=\"output_layer\"))\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=loss, \n",
    "                  metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bf9825-4224-4b27-9a21-b058fa42a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_model_2_pt2, improved_model_2_pt2_history = fit_model(create_model_two_pt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3159e36d-ac17-4a18-8ce3-3b3a020e60e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_model_2_pt2_df = create_model_training_df(improved_model_2_pt2_history.history, 10)\n",
    "plot_model_performance(improved_model_2_pt2_df, \"Bidirectional LSTM + LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec7c806-198e-4ed8-bfa5-49c042fa1e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(improved_model_2_pt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102fa6b9-2e60-48d1-b35e-f00e087c646e",
   "metadata": {},
   "source": [
    "# Improving Model 2 Pt.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a18ba6-6e18-4aa1-b684-22605b8b01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_self_attention import SeqSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c015f3-2297-4588-8ce6-95823a03a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_two_pt3(rnn_units=200,\n",
    "                         optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                         loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                         metrics=[\"accuracy\"]):\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding_layer = Embedding(vocab_size + 1, \n",
    "                                EMBEDDING_DIM, \n",
    "                                weights=[embeddings_matrix], \n",
    "                                input_length=MAX_SEQUENCE_LENGTH, \n",
    "                                trainable=True) \n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    model.add(layers.Input(shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "                           dtype=\"int32\"))\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "    \n",
    "    model.add(layers.Bidirectional(layers.LSTM(rnn_units,  \n",
    "                                               return_sequences=True)))\n",
    "    \n",
    "    model.add(SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                               kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "                               bias_regularizer=keras.regularizers.l1(1e-4),\n",
    "                               attention_regularizer_weight=1e-4,))\n",
    "    \n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(layers.GlobalAvgPool1D())\n",
    "    \n",
    "    model.add(layers.Dense(50))\n",
    "    \n",
    "    model.add(layers.Dense(num_labels, \n",
    "                           activation=\"softmax\"))\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=loss, \n",
    "                  metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67451ae-0e48-4a5c-a6d0-423b382727e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_model_2_pt3, improved_model_2_pt3_history = fit_model(create_model_two_pt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8199c9c-73cd-4988-89b3-b486198f53c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_model_2_pt3_df = create_model_training_df(improved_model_2_pt3_history.history, 10)\n",
    "plot_model_performance(improved_model_2_pt3_df, \"Bidirectional LSTM with Average Pooling \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a495dec7-5848-4cd0-8be0-a91178494434",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(improved_model_2_pt3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66600a3a-acec-4c4e-937a-cfdc689e622d",
   "metadata": {},
   "source": [
    "# Improving Model 2 Pt.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7dbddf-e756-4bc0-b942-71404448f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_two_pt4(rnn_units=200,\n",
    "                         attn_units=100,\n",
    "                         optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                         loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                         metrics=[\"accuracy\"]):\n",
    "    \n",
    "    sequence_input = layers.Input(shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "                                  dtype=\"int32\")\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding_layer = Embedding(vocab_size + 1, \n",
    "                                EMBEDDING_DIM, \n",
    "                                weights=[embeddings_matrix], \n",
    "                                input_length=MAX_SEQUENCE_LENGTH, \n",
    "                                trainable=True)(sequence_input) \n",
    "    \n",
    "    \n",
    "    lstm_out, hidden_h, hidden_c, back_h, back_c = layers.Bidirectional(layers.LSTM(rnn_units, \n",
    "                                                                        return_sequences=True, \n",
    "                                                                        return_state=True))(embedding_layer)\n",
    "    \n",
    "    x, attn_weights = Attention(attn_units)(lstm_out, hidden_h)\n",
    "\n",
    "    \n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    \n",
    "    output = layers.Dense(num_labels,\n",
    "                          activation=\"softmax\")(x)\n",
    "    \n",
    "\n",
    "    model = keras.Model(inputs=sequence_input, outputs=output)    \n",
    "    \n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=loss, \n",
    "                  metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c1ce0-343a-4bb6-88dc-5744b1c24057",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_model_2_pt4, improved_model_2_pt4_history = fit_model(create_model_two_pt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d419b2c-24c7-4a09-afc7-3bf8ab5fe22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_model_2_pt4_df = create_model_training_df(improved_model_2_pt4_history.history, 10)\n",
    "plot_model_performance(improved_model_2_pt4_df, \"Bidirectional LSTM (w Attention)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a474ab8c-b798-4c61-8a37-e4802b2bdea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(improved_model_2_pt4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
